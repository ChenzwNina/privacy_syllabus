{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "044f1a64-44e1-4cdf-9502-026f00ce5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and process multiple URLs in the \"Links\" column\n",
    "def process_links_column(links_cell):\n",
    "    if pd.isna(links_cell):  # Check if the cell is empty\n",
    "        return []\n",
    "    \n",
    "    # Split the cell content by ', ' to extract multiple URLs\n",
    "    urls = links_cell.split(\", \")\n",
    "    \n",
    "    # Process each URL (if needed, you can add more logic here)\n",
    "    processed_urls = [url.strip() for url in urls]  # Optionally, strip whitespace around the URLs\n",
    "    return processed_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b760650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get content from the syllabus website and replace url with abbreviation\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def complete_url(path, base):\n",
    "    if not path.startswith(('http', 'https')):\n",
    "        # Combine the domain and path if it starts with \"/\"\n",
    "        return urljoin(base, path)\n",
    "    else:\n",
    "        # Return the path as is if it doesn't start with \"/\"\n",
    "        return path\n",
    "\n",
    "def parse_website_with_mappings(url):\n",
    "    url_mapping = {}\n",
    "    reason = \"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Parse the webpage with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # Counter for URL abbreviation\n",
    "        url_counter = 1\n",
    "        \n",
    "        # Iterate over all anchor tags with href attributes\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            \n",
    "            href_updated = complete_url(href, url)\n",
    "            print(href_updated)\n",
    "            \n",
    "            # Create a unique abbreviation for the URL (e.g., url_1, url_2)\n",
    "            abbreviation = f\"url_{url_counter}\"\n",
    "            url_mapping[abbreviation] = href_updated\n",
    "    \n",
    "            # Modify the link text to include the abbreviation in parentheses\n",
    "            link.string = f\"{link.get_text()} ({abbreviation})\"\n",
    "    \n",
    "            # Increment the URL counter for the next abbreviation\n",
    "            url_counter += 1\n",
    "    \n",
    "        # Convert the modified soup object back to a string (HTML with modifications)\n",
    "        modified_content = soup.get_text()\n",
    "\n",
    "        cleaned_content = re.sub(r'\\n+', '\\n', modified_content)\n",
    "        \n",
    "        return cleaned_content, url_mapping, reason\n",
    "\n",
    "    except requests.exceptions.SSLError:\n",
    "        modified_content = \"error\"\n",
    "        reason = \"SSL error\"\n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        modified_content = \"error\"\n",
    "        reason = \"Request error\"\n",
    "        \n",
    "    except Exception:\n",
    "        modified_content = \"error\"\n",
    "        reason = \"Parsing error - need further investigation\"\n",
    "        \n",
    "    return modified_content, url_mapping, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1c4508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Google file locally\n",
    "\n",
    "import gdown\n",
    "\n",
    "def google_access(url, class_folder, class_id):\n",
    "    google_file_id = url.split('/d/')[1].split('/')[0]\n",
    "    \n",
    "    if \"document\" in url:\n",
    "        #Download the doc as a PDF\n",
    "        download_url = f\"https://docs.google.com/document/d/{google_file_id}/export?format=pdf\"\n",
    "    elif \"file\" in url:\n",
    "        download_url = f\"https://drive.google.com/uc?id={google_file_id}\"\n",
    "    else:\n",
    "        output_path = \"error\"\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.makedirs(class_folder)\n",
    "    \n",
    "    output_path = f\"{os.path.join(class_folder)}/{class_id}_syllabus.pdf\"\n",
    "\n",
    "    # Download the file using gdown\n",
    "    gdown.download(download_url, output=output_path, quiet=False)\n",
    "    \n",
    "    print(f\"Download url: {download_url}\")\n",
    "    print(f\"File downloaded successfully to: {output_path}\")\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "608c9e5b-2208-4293-b52d-2d549eebe780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PDF and save it locally\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_pdf(url, class_folder, class_id):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(class_folder):\n",
    "            os.makedirs(class_folder)\n",
    "\n",
    "        # Define the full path to save the PDF\n",
    "        output_path = os.path.join(class_folder, f\"{class_id}_syllabus.pdf\")\n",
    "        \n",
    "        # Save the PDF content to the file\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "            \n",
    "        print(f\"PDF successfully downloaded and saved to {local_path}\")\n",
    "    else:\n",
    "        output_path = \"error\"\n",
    "        print(\"PDF fails to be downloaded\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b02f9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF content from downloaded syllabus and PDF\n",
    "\n",
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf_with_spaces(pdf_file_path):\n",
    "    url_mapping = {}\n",
    "    full_text = \"\"\n",
    "    url_counter = 1\n",
    "\n",
    "    # Open the PDF file using pdfplumber\n",
    "    with pdfplumber.open(pdf_file_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                full_text += page_text + \"\\n\"\n",
    "\n",
    "            # Extract annotations (like hyperlinks) from the page\n",
    "            if page.annots:\n",
    "                for annot in page.annots:\n",
    "                    if annot.get(\"uri\"):\n",
    "                        url = annot[\"uri\"]\n",
    "                        abbreviation = f\"url_{url_counter}\"\n",
    "                        url_mapping[abbreviation] = url\n",
    "                        full_text += f\" [{abbreviation}]\"\n",
    "                        url_counter += 1\n",
    "\n",
    "    # Display the updated content and the URL mapping\n",
    "    print(\"Updated PDF Content:\\n\")\n",
    "    print(full_text)\n",
    "    print(\"\\nURL Mapping:\\n\")\n",
    "    print(url_mapping)\n",
    "    return full_text, url_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9685f64c-ea1e-4260-a318-09f3ed736b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get syllabus url and handle it based on its type: Google Drive PDF, Google Doc, web PDF or a website.\n",
    "\n",
    "def get_content_and_url(url, class_folder, class_id):\n",
    "    url_mappings = {}\n",
    "    reason = \"\"\n",
    "    print(url)\n",
    "    \n",
    "    if \"google\" in url.lower() or \".pdf\" in url.lower():\n",
    "        if \"google\" in url.lower():\n",
    "            # Download the Google file to local folder as PDF and get its path\n",
    "            output_path = google_access(url, class_folder, class_id)\n",
    "\n",
    "            if output_path == \"error\":\n",
    "                syllabus_content = \"error\"\n",
    "                reason = \"Google downloading error\"\n",
    "        else:\n",
    "            #Download the PDF to local folder and get its path\n",
    "            print(\"this goes to PDF downloading function\")\n",
    "            output_path = download_pdf(url, class_folder, class_id)\n",
    "\n",
    "            if output_path == \"error\":\n",
    "                syllabus_content = \"error\"\n",
    "                reason = \"PDF downloading error\"\n",
    "\n",
    "        # Get content from downloaded Google files or PDF\n",
    "        if output_path != \"error\":\n",
    "            # Get PDF content\n",
    "            syllabus_content, url_mappings = extract_text_from_pdf_with_spaces(output_path)\n",
    "            \n",
    "    # If it is a website\n",
    "    else: \n",
    "        syllabus_content, url_mappings, reason = parse_website_with_mappings(url)\n",
    "        \n",
    "    return syllabus_content, url_mappings, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "327e4d27-c8e8-44dc-a5c1-5fe1f96592da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run reading identification and extraction with API\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key= \"sk-None-dRO71h3oY2mP3vVnR6bcT3BlbkFJiGFnZrrLRmq1s14E6q4s\") #API key here\n",
    "\n",
    "# Load system_prompt from local files\n",
    "def load_system_prompt():\n",
    "    file_path = \"/Users/ninachen/Desktop/reading_extract/Find Reading Prompt text.txt\"\n",
    "\n",
    "    # Open the file in read mode and load the content into a variable\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "def split_text_by_words(text, max_words=500):\n",
    "    words = text.split()  # Split the text into words\n",
    "    # If the number of words is less than or equal to max_words, return the text as one chunk\n",
    "    if len(words) <= max_words:\n",
    "        return [text]  # No need to split, return the text in a list with one item\n",
    "    \n",
    "    # Otherwise, create chunks of up to max_words\n",
    "    chunks = [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "    return chunks\n",
    "\n",
    "# Use API to run reading extraction\n",
    "def api_syllabus_classification(syllabus_content):\n",
    "\n",
    "    chunks = split_text_by_words(syllabus_content, max_words=500)\n",
    "\n",
    "    chunk_number = len(chunks)\n",
    "    \n",
    "    results= []\n",
    "\n",
    "    for chunk in chunks:\n",
    "    #Run GPT-4 for the first round\n",
    "        response = client.chat.completions.create(\n",
    "            model= \"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": load_system_prompt()},\n",
    "                {\"role\": \"user\", \"content\": f\"Identify all readings mentioned in the syllabus:{chunk}\"}\n",
    "              ],\n",
    "            temperature = 0\n",
    "            )\n",
    "        results.append(response.choices[0].message.content)\n",
    "\n",
    "    return results, chunk_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4258c37-736f-4fa2-b65b-f977121ae398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export reading as a spreadsheet and add class ID.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def export_reading_syllabus(results, class_id, class_folder, url_mappings, url_count, chunk_number):\n",
    "\n",
    "    total_df = pd.DataFrame(columns=['Reading Name', 'Author Name', 'Published Year', 'Page Numbers to Read', 'URL'])\n",
    "    \n",
    "    for i in range(chunk_number):\n",
    "        reading = json.loads(results[i].replace(\"```json\",\"\").replace(\"```\",\"\").replace(\"\\n\",\"\"))\n",
    "        df = pd.DataFrame(reading)\n",
    "        total_df = pd.concat([total_df, df], ignore_index=True) \n",
    "\n",
    "    #Count the number of rows in the DataFrame\n",
    "    num_rows = len(total_df)\n",
    "    \n",
    "    #Add class_id in the new column\n",
    "    total_df['Class ID'] = [f\"{class_id}\"] * num_rows\n",
    "\n",
    "    # Create a folder if it has not been created yet.\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.makedirs(class_folder)\n",
    "\n",
    "    # Replace the URL abbreviations with the actual URLs from the url_mapping dictionary\n",
    "    total_df['URL'] = total_df['URL'].apply(lambda x: url_mappings.get(x, x))  # Replace the abbreviation with the actual URL\n",
    "\n",
    "        \n",
    "    # Save to an Excel spreadsheet\n",
    "    output_file = f\"{class_folder}/{class_id}_readings_{url_count}.xlsx\"\n",
    "    total_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff09cb9a-2685-4f98-bfda-1771c24a2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two error lists and output as a spreadsheet.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def merge_error_lists(list1, list2, class_folder, class_id):\n",
    "    # Create a folder if it has not been created yet.\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.makedirs(class_folder)\n",
    "\n",
    "    # Define the full path to save the error spreadsheet\n",
    "    error_output_path = os.path.join(class_folder, f\"{class_id}_error_output.xlsx\")\n",
    "    \n",
    "    # Step 1: Create DataFrames\n",
    "    if not list1:\n",
    "        df1 = pd.DataFrame()  # Create an empty DataFrame if the first list is empty\n",
    "    else:\n",
    "        df1 = pd.DataFrame(list1)\n",
    "        df1['Class ID'] = class_id  # Assign the class_id to all rows in df1\n",
    "\n",
    "    if not list2:\n",
    "        df2 = pd.DataFrame()  # Create an empty DataFrame if the second list is empty\n",
    "    else:\n",
    "        df2 = pd.DataFrame(list2)\n",
    "        df2['Note'] = \"Formatting Error\"  # Add a 'Note' column to df2\n",
    "        df2['Class ID'] = class_id  # Assign the class_id to all rows in df2\n",
    "\n",
    "    # Concatenate the two DataFrames vertically (stack them)\n",
    "    merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Output the merged DataFrame to a spreadsheet\n",
    "    merged_df.to_excel(error_output_path, index=False)\n",
    "    print(f\"Merged DataFrame saved to {error_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95a63510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class55\n",
      "https://web.stanford.edu/class/cs384/\n",
      "https://web.stanford.edu/class/cs384/index.html\n",
      "https://web.stanford.edu/class/cs384/index.html#schedule\n",
      "https://web.stanford.edu/class/cs384/index.html#logistics\n",
      "https://web.stanford.edu/class/cs384/project.html\n",
      "https://edstem.org/us/courses/38694/discussion/\n",
      "http://nlp.stanford.edu/\n",
      "http://stanford.edu/\n",
      "https://web.stanford.edu/~jurafsky/\n",
      "https://riakalluri.com/\n",
      "https://www.peterhenderson.co/\n",
      "https://calendly.com/jurafsky\n",
      "https://calendly.com/riakall\n",
      "https://calendly.com/phend-1/office-hours-1\n",
      "https://oae.stanford.edu/accommodations/academic-accommodations\n",
      "https://web.stanford.edu/class/cs384/project.html\n",
      "https://oae.stanford.edu/faculty-teaching-staff/syllabus-statement\n",
      "http://oae.stanford.edu\n",
      "https://vaden.stanford.edu/caps-and-wellness\n",
      "https://web.stanford.edu/class/cs384/slides/384-lec1.pptx\n",
      "https://web.stanford.edu/class/cs384/slides/384-lec1.pdf\n",
      "https://web.stanford.edu/class/cs384/slides/384-lec2.irb.pptx\n",
      "https://web.stanford.edu/class/cs384/slides/384-lec2.irb.pdf\n",
      "https://www.hhs.gov/ohrp/sites/default/files/the-belmont-report-508c_FINAL.pdf\n",
      "https://anatomyof.ai/\n",
      "https://time.com/6247678/openai-chatgpt-kenya-workers/\n",
      "https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00041\n",
      "https://journals-sagepub-com.stanford.idm.oclc.org/doi/pdf/10.1177/2056305118763366\n",
      "https://arxiv.org/pdf/1803.09010.pdf\n",
      "https://terpconnect.umd.edu/~kshilton/pdf/VitaketalCSCWpreprint.pdf\n",
      "http://doi.org/10.1177/0038038517708140\n",
      "https://www.nejm.org/doi/full/10.1056/NEJM199711133372006\n",
      "https://www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/index.html\n",
      "https://arxiv.org/abs/2005.13213\n",
      "https://www-jstor-org.stanford.idm.oclc.org/stable/pdf/4168760.pdf?refreqid%3Dexcelsior%253A29d14d7dc864b96abb93f4f53f119b95\n",
      "https://arxiv.org/pdf/2004.09095.pdf\n",
      "https://aclanthology.org/2020.coling-main.313.pdf\n",
      "https://aclanthology.org/2020.findings-emnlp.195.pdf\n",
      "http://www.jstor.org/stable/20024652\n",
      "https://arxiv.org/pdf/2303.18190.pdf\n",
      "https://aclanthology.org/2022.acl-long.539.pdf\n",
      "https://link.springer.com/article/10.1007/s13347-020-00405-8\n",
      "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00447/109285/Quality-at-a-Glance-An-Audit-of-Web-Crawled\n",
      "https://aclanthology.org/2022.acl-short.82.pdf\n",
      "https://aclanthology.org/2022.findings-acl.184.pdf\n",
      "https://aclanthology.org/2022.aacl-main.62/\n",
      "https://aclanthology.org/2022.lrec-1.691.pdf\n",
      "https://aclanthology.org/P17-2009.pdf\n",
      "https://arxiv.org/abs/2304.11094\n",
      "https://www-pnas-org.stanford.idm.oclc.org/content/early/2020/03/17/1915768117\n",
      "https://dl.acm.org/doi/10.1145/3442188.3445922\n",
      "https://dl.acm.org/doi/pdf/10.1145/3531146.3533088\n",
      "https://arxiv.org/pdf/2112.04359.pdf\n",
      "https://arxiv.org/pdf/2210.05791.pdf\n",
      "https://aclanthology.org/2022.sigdial-1.4.pdf\n",
      "https://arxiv.org/pdf/2209.07858.pdf\n",
      "https://www.nature.com/articles/s42256-021-00359-2\n",
      "https://arxiv.org/pdf/2212.08061.pdf\n",
      "https://arxiv.org/abs/2302.07459\n",
      "https://arxiv.org/abs/2303.08721\n",
      "https://arxiv.org/pdf/2302.00560.pdf\n",
      "https://arxiv.org/pdf/2304.11082.pdf\n",
      "https://aclanthology.org/2022.naacl-main.92.pdf\n",
      "https://arxiv.org/abs/2304.02819\n",
      "https://arxiv.org/pdf/2304.11111.pdf\n",
      "https://aclanthology.org/2022.findings-acl.165.pdf\n",
      "https://aclanthology.org/N18-2002.pdf\n",
      "https://aclanthology.org/2020.findings-emnlp.301.pdf\n",
      "https://aclanthology.org/2021.acl-long.416.pdf\n",
      "https://aclanthology.org/2021.tacl-1.84.pdf\n",
      "https://arxiv.org/pdf/2204.05862.pdf\n",
      "https://arxiv.org/abs/2301.04246\n",
      "https://doi.org/10.31219/osf.io/stakv\n",
      "https://aclanthology.org/2022.nlp4pi-1.11.pdf\n",
      "https://doi.org/10.1145/3308560.3316735\n",
      "https://dl.acm.org/doi/pdf/10.1145/3308560.3316494\n",
      "https://www.washingtonpost.com/technology/2023/04/16/russia-disinformation-discord-leaked-documents/\n",
      "https://link.springer.com/article/10.1007/s00146-020-00950-y\n",
      "https://arxiv.org/abs/2303.15715\n",
      "https://www.wired.com/story/chatgpt-generative-artificial-intelligence-regulation/\n",
      "https://dl.acm.org/doi/pdf/10.1145/3442188.3445885\n",
      "https://hai.stanford.edu/news/radical-proposal-data-cooperatives-could-give-us-more-power-over-our-data\n",
      "https://www.nytimes.com/interactive/2023/03/01/arts/design/quiz-copyright.html?action=click&module=RelatedLinks&pgtype=Article\n",
      "https://www.nytimes.com/2023/03/01/arts/design/appropriation-warhol-renaissance-copyright.html\n",
      "https://arxiv.org/pdf/2302.10870.pdf\n",
      "https://stablediffusionlitigation.com/pdf/00201/1-1-stable-diffusion-complaint.pdf\n",
      "https://texaslawreview.org/fair-learning/\n",
      "https://scholarship.law.cornell.edu/facpub/1481/\n",
      "https://arxiv.org/pdf/2202.07646.pdf\n",
      "https://web.stanford.edu/class/cs384/opportunitiesandriskssection5.4.pdf\n",
      "https://arxiv.org/pdf/2108.07258.pdf\n",
      "https://www2.law.ucla.edu/volokh/ailibel.pdf\n",
      "https://reason.com/volokh/2023/03/27/are-ai-program-outputs-reasonably-perceived-as-factual-a-response-to-eugene/\n",
      "https://techcrunch.com/2023/04/12/chatgpt-italy-gdpr-order/amp/\n",
      "https://hai.stanford.edu/news/regulating-ai-through-data-privacy\n",
      "https://www.brookings.edu/research/protecting-privacy-in-an-ai-driven-world/\n",
      "https://www.cbc.ca/news/world/openai-chatgpt-data-privacy-investigations-1.6804205\n",
      "https://arxiv.org/abs/2202.05520\n",
      "https://web.stanford.edu/class/cs384/restricted/klein23.pdf\n",
      "https://www.nytimes.com/2023/04/16/opinion/this-is-too-important-to-leave-to-microsoft-google-and-facebook.html?searchResultPosition=4\n",
      "https://www.whitehouse.gov/ostp/ai-bill-of-rights/\n",
      "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/\n",
      "https://ainowinstitute.org/news/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act\n",
      "https://www.brookings.edu/research/the-eu-and-us-diverge-on-ai-regulation-a-transatlantic-comparison-and-steps-to-alignment/\n",
      "https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence\n",
      "https://ai.objectives.institute/whitepaper\n",
      "https://foundation.mozilla.org/en/initiatives/great-tech-great-responsibility/\n",
      "https://gizmodo.com/google-plans-not-to-renew-its-contract-for-project-mave-1826488620\n",
      "https://slideslive.com/38943464/resistance-and-the-five-stages-of-corporate-grief\n",
      "https://futureoflife.org/open-letter/pause-giant-ai-experiments/\n",
      "https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/\n",
      "https://web.stanford.edu/class/cs384/restricted/linakhan.pdf\n",
      "https://www.nytimes.com/2023/05/03/opinion/ai-lina-khan-ftc-technology.html?smid=nytcore-ios-share&referringSource=articleShare\n",
      "https://www.bbc.com/news/world-us-canada-65452940\n",
      "https://hackingsemantics.xyz/2023/closed-baselines/\n",
      "https://www.youtube.com/watch?v=BY9KV8uCtj4&t=3s\n",
      "https://futureoflife.org/wp-content/uploads/2023/04/FLI_Policymaking_In_The_Pause.pdf\n",
      "https://www.newyorker.com/science/annals-of-artificial-intelligence/will-ai-become-the-new-mckinsey\n",
      "https://iclr.cc/virtual_2020/speaker_3.html\n",
      "https://spectrum.library.concordia.ca/id/eprint/986506/7/Indigenous_Protocol_and_AI_2020.pdf\n",
      "https://www.indigenous-ai.net/position-paper\n",
      "https://alliedmedia.org/wp-content/uploads/2020/09/peoples-guide-ai.pdf\n",
      "https://docs.google.com/presentation/d/1yZRu12NL04I67RnP3n4nJZ3eJqhcVhJ8TErDW-WGUgQ/preview?slide=id.g7d5d986579_0_0\n",
      "https://arxiv.org/pdf/1912.04883.pdf\n",
      "http://www.pnas.org/content/early/2017/05/30/1702413114\n",
      "http://users.nber.org/~dlchen/papers/Gender_Attitudes_in_the_Judiciary_AEJApplied.pdf\n",
      "https://arxiv.org/pdf/2302.07268.pdf\n",
      "https://www.pnas.org/doi/pdf/10.1073/pnas.2120510119\n",
      "https://www.pnas.org/doi/abs/10.1073/pnas.2121798119\n",
      "class56\n",
      "https://jnear.github.io/cs3110-data-privacy/\n",
      "https://jnear.github.io/cs3110-data-privacy/\n",
      "https://jnear.github.io/cs3110-data-privacy/#course-description\n",
      "https://jnear.github.io/cs3110-data-privacy/#administrative\n",
      "https://jnear.github.io/cs3110-data-privacy/#resources\n",
      "https://jnear.github.io/cs3110-data-privacy/#textbook---other-references\n",
      "https://jnear.github.io/cs3110-data-privacy/#policies\n",
      "https://jnear.github.io/cs3110-data-privacy/#grading\n",
      "https://jnear.github.io/cs3110-data-privacy/#exams---quizzes\n",
      "https://jnear.github.io/cs3110-data-privacy/#homework-assignments-and-in-class-exercises\n",
      "https://jnear.github.io/cs3110-data-privacy/#late-work\n",
      "https://jnear.github.io/cs3110-data-privacy/#collaboration--allowed-references\n",
      "https://jnear.github.io/cs3110-data-privacy/#final-projects\n",
      "https://jnear.github.io/cs3110-data-privacy/#cs-student-research-day--extra-credit\n",
      "https://jnear.github.io/cs3110-data-privacy/#schedule\n",
      "https://programming-dp.com\n",
      "https://github.com/uvm-plaid/programming-dp/blob/master/book.pdf\n",
      "https://brightspace.uvm.edu/d2l/home/89959\n",
      "https://brightspace.uvm.edu/d2l/home/89961\n",
      "https://github.com/jnear/cs3110-data-privacy\n",
      "https://github.com/jnear/cs3110-data-privacy/tree/master/exercises\n",
      "https://github.com/jnear/cs3110-data-privacy/tree/master/homework\n",
      "https://github.com/jnear/cs3110-data-privacy/tree/master/slides\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/master/slides/exam1-review.md\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/master/slides/exam2-review.md\n",
      "https://programming-dp.com\n",
      "https://github.com/uvm-plaid/programming-dp/blob/master/book.pdf\n",
      "https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf\n",
      "https://privacytools.seas.harvard.edu/files/privacytools/files/pedagogical-document-dp_new.pdf\n",
      "https://jnear.github.io/cs3110-data-privacy/jupyter\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/master/slides/formulas.pdf\n",
      "https://www3.nd.edu/~rwilliam/stats1/x11.pdf\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/master/slides/Intro%20to%20machine%20learning.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/master/slides/privacy_definitions.pdf\n",
      "https://jnear.github.io/cs3110-data-privacy/jupyter\n",
      "https://www.uvm.edu/policies/student/acadintegrity.pdf\n",
      "https://jnear.github.io/cs3110-data-privacy/projects\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_1.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_2.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_3.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_4.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_5.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_6.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_7.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_8.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_9.ipynb\n",
      "https://github.com/jnear/cs3110-data-privacy/blob/main/homework/CS3110_HW_10.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Import class spreadsheet\n",
    "input_file_path = '/Users/ninachen/Desktop/reading_extract_spreadsheet/Class_Reading_Mapping.xlsx'\n",
    "df = pd.read_excel(input_file_path, engine='openpyxl')\n",
    "\n",
    "# Create an empty list to save syllabus that cannot be processed\n",
    "error_syllabus = [] #Parsing or downloading issue\n",
    "error_format_syllabus = [] #Json format issue\n",
    "\n",
    "#Folder location to download Google Files and PDF.\n",
    "save_directory = \"/Users/ninachen/Desktop/extract_with_text_chunks/\"\n",
    "\n",
    "#Process each row individually\n",
    "for index, row in df.iloc[54:56].iterrows():\n",
    "    class_id = row['Class ID']\n",
    "    print(class_id)\n",
    "    \n",
    "    links = row['Links']\n",
    "    # Get all URLs in this cell\n",
    "    urls_in_row = process_links_column(links)\n",
    "\n",
    "    # Download directory \n",
    "    class_folder = os.path.join(save_directory, class_id)\n",
    "\n",
    "    url_count = 0\n",
    "\n",
    "    if urls_in_row:\n",
    "        for url in urls_in_row:\n",
    "            # Get syllabus content and url mappings.\n",
    "            syllabus_with_url, syllabus_url_mappings, reason = get_content_and_url(url, class_folder, class_id)\n",
    "        \n",
    "            # If the content extraction failed, append it to the error list.\n",
    "            if syllabus_with_url == \"error\":\n",
    "                row['Note'] = reason  # Add the 'Failed Reason' field to the row\n",
    "                error_syllabus.append(row)\n",
    "            else:\n",
    "                # If the content extraction succeeded, run reading extraction API.\n",
    "                syllabus_reading_list, chunk_number = api_syllabus_classification(syllabus_with_url)\n",
    "                \n",
    "                # Export reading extraction into a spreadsheet\n",
    "                export_reading_syllabus(syllabus_reading_list, class_id, class_folder, syllabus_url_mappings, url_count, chunk_number)\n",
    "            url_count = url_count + 1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Convert the list of JSON objects to a DataFrame\n",
    "df1 = pd.DataFrame(error_syllabus)\n",
    "df1.to_excel(f\"/Users/ninachen/Desktop/reading_extract_spreadsheet/_error.xlsx\", index=False)\n",
    "\n",
    "\n",
    "    #     # If the reading extraction failed, append it to the format error list.\n",
    "    #     error_format_syllabus.append(row)\n",
    "\n",
    "    # # Merge two error lists and output an error spreadsheet if there is any\n",
    "    # merge_error_lists(error_format_syllabus, error_syllabus, class_folder, class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c1675b9-6cb6-4915-b180-55ad26c615e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCS 3110/5110 Data Privacy | cs3110-data-privacy\\ncs3110-data-privacy (url_1)\\nUVM CS 3110/5110: Data Privacy (Fall 2024)\\nCourse Description (url_2)\\nAdministrative (url_3)\\nResources (url_4)\\nTextbook & Other References (url_5)\\nPolicies (url_6)\\nGrading (url_7)\\nExams & Quizzes (url_8)\\nHomework Assignments and In-class Exercises (url_9)\\nLate Work (url_10)\\nCollaboration & Allowed References (url_11)\\nFinal Projects (url_12)\\nCS Student Research Day & Extra Credit (url_13)\\nSchedule (url_14)\\nCourse Description\\nHow can we learn from sensitive data collected from individuals, while protecting the privacy of those individuals?\\nThis question is central to the study of data privacy,\\nand is increasingly relevant with the widespread collection of our personal data.\\nAnalysis of this data can lead to important benefits for society,\\nincluding advances in medicine and public infrastructure,\\nbut can also result in privacy breaches that expose our most closely-held secrets.\\nThis course will explore both threats to privacy and solutions to the data privacy problem.\\nWe will demonstrate that traditional approaches to protecting privacy, such as anonymization,\\nare subject to powerful attacks that reveal individuals’ sensitive data.\\nWe will see that while more recent approaches for protecting privacy, including k-anonymity and l-diversity,\\nare more resistant to these attacks, they are not immune.\\nThen, we will explore recent formal notions of privacy, including differential privacy.\\nDifferential privacy provides a rigorous formal definition of individual privacy that enables a wide range of\\nstatistical analyses while protecting privacy.\\nWe will explore a number of differentially private algorithms for analytics and machine learning,\\nand learn about the algorithmic building blocks and proof techniques used to develop them.\\nIn addition to learning about the mathematical foundations of differential privacy,\\nwe will explore its practical implications.\\nWe will learn about existing practical systems for enforcing differential privacy and examine the challenges of building such systems.\\nThis course will include programming assignments and an end-of-semester project,\\nin which students are expected to demonstrate both mastery of the concepts we explore and understanding of\\ntheir practical implications by building their own systems that perform privacy-preserving analyses on real data.\\nLearning Objectives\\nBy the end of this course, you will be able to:\\nDescribe the problem and challenges of data privacy\\nConduct a privacy attack on de-identified data\\nDefine and apply formal notions of privacy, including k-Anonymity and differential privacy\\nDesign differentially private algorithms and argue that they are correct\\nEvaluate the accuracy and efficiency properties of differentially private algorithms\\nAdministrative\\nLecture: Monday, Wednesday, Friday, 1:10pm - 2:00pm, at Dewey Hall 314\\nInstructor: Joe Near (jnear at uvm dot edu)\\nGraduate Teaching Assistant: TBA\\nOffice hours:\\n    \\nJoe Near (instructor): Mondays and Fridays, 9:30am-10:30am, and by appointment; Innovation Hall E458 (or MS Teams)\\nResources\\nCourse textbook is available online (url_15) or as a PDF (url_16)\\nBrightspace for the course:\\n    \\nCS 3110 (undergrad section) (url_17)\\nCS 5110 (graduate section) (url_18)\\nCourse Github Repo is available here (url_19)\\nWeekly exercises\\nDownload exercises here (url_20)\\nTurn in notebook files on Brightspace\\nHomework assignments\\nDownload notebooks here (url_21)\\nTurn in notebook files on Brightspace\\nDiscussions will take place on MS Teams\\nSlides from lecture are available here (url_22)\\nReview Sheets for exams:\\n    \\nExam 1 (url_23)\\nExam 2 (url_24)\\nTextbook & Other References\\nPlease do not buy any books for this course. All required reference material is available online for free.\\nThe primary textbook we will use for this course is:\\nProgramming Differential Privacy (url_25)\\nJoseph P. Near and Chiké Abuah.\\nAlso available as a PDF (url_26)\\nThe following resources may also be useful for additional reading:\\n[D&R] The Algorithmic Foundations of Differential Privacy (url_27)\\nCynthia Dwork and Aaron Roth.\\n[Nissim] Differential Privacy: A Primer for a Non-technical Audience (url_28)\\n Kobbi Nissim, Thomas Steinke, Alexandra Wood, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, David R. O’Brien, and Salil Vadhan.\\nIn addition to these, we will reference a number of academic papers throughout the semester (especially for the section on privacy-preserving machine learning).\\nLinks and Helpful Resources\\nHow to set up Jupyter Notebooks (url_29)\\nDefinitions and formulas (url_30) that may be helpful on quizzes and exams\\nNotes on probability distributions (url_31)\\nIntro to differentially private machine learning (url_32)\\nVariants of differential privacy (url_33)\\nPolicies\\nGrading\\nYour grade for the course will be determined as follows:\\n10 homework assignments (5% each; 50% total)\\nin-class weekly exercises (20% total)\\nmidterm exam (10%)\\nfinal exam (10%)\\nfinal project (10%)\\nYour final grade will be determined by summing the total number of\\npoints awarded and calculating the percentage of the total possible\\npoints. This percentage is translated into a letter grade as follows:\\nUndergraduate Students\\nPercent\\nLetter Grade\\n98-100\\nA+\\n93-97\\nA\\n90-92\\nA-\\n87-89\\nB+\\n83-86\\nB\\n80-82\\nB-\\n77-79\\nC+\\n73-76\\nC\\n70-72\\nC-\\n67-69\\nD+\\n63-66\\nD\\n60-62\\nD-\\n<60\\nF\\nGraduate Students\\nPercent\\nLetter Grade\\n98-100\\nA+\\n93-97\\nA\\n90-92\\nA-\\n87-89\\nB+\\n83-86\\nB\\n80-82\\nB-\\n77-79\\nC+\\n73-76\\nC\\n70-72\\nC-\\n<70\\nF\\nExams & Quizzes\\nThere will be two exams: a midterm and a final. You will be allowed unlimited notes for each exam (but please don’t print a whole book). See the schedule below for the dates.\\nHomework Assignments and In-class Exercises\\nThis course will use Python for examples and for programming\\nassignments.  Students are expected to be proficient in Python\\nprogramming.  Programming assignments will be distributed and turned\\nin as Jupyter notebooks. Click\\nhere (url_34) for\\ninstructions on installing Jupyter Notebook.\\nAssignment Submission: Homework and in-class exercises will be\\nturned in via Brightspace.\\nTo submit an assignment:\\nComplete the released Jupyter Notebook by filling in answers to all the questions\\nSubmit the notebook file (the .ipynb file) as your solution on Brightspace\\nPlease do not change the name of the .ipynb file. This makes the\\ngrading process more difficult.\\nPlease let me know if you have any questions about the submission process.\\nGrading rubric:\\n100% - Correct or with minor issues\\nCode: runs without error, appears correct or with only minor issues\\nWritten: coherent and correct, perhaps with minor details missing\\n75% - Main idea on the right path, with parts incorrect\\nCode: appears complete and appears to implement the right idea;\\ncode may throw errors and give incorrect results\\nWritten: gets the main idea mostly correct; addresses all or nearly\\nall of the required points; may contain some conceptual errors\\n50% - Decent start, but misses the main idea\\nCode: appears to attempt to implement an approximation of the right\\nidea; code may be incomplete and not run at all\\nWritten: attempts to approximate the main idea; may avoid\\naddressing many required points; may contain major conceptual\\nerrors\\n0% - Missing/no answer\\nSolutions and feedback: Homework solutions will be posted on\\nBrightspace under “homework solutions.” Grades will be posted on\\nBrightspace. To see your graded assignment, visit the following link:\\nhttps://jnear.w3.uvm.edu/cs3110_feedback/<your-netid-here>\\nReplace <your-netid-here> with your actual netid. You will need to\\nlog in using your UVM credentials to view your graded assignments. If\\nyou have questions about how a question was graded, or if you spot a\\nmistake in grading, please let me know.\\nLate Work\\nLate work may be accepted, but you must make arrangements with me\\nfirst. If you need to turn something in late, for any reason, please\\nemail me before the deadline. Depending on the circumstances, I may\\n(or may not) impose a late penalty on your grade.\\nCollaboration & Allowed References\\nCollaboration on the high-level ideas and approach on assignments is encouraged.\\nCopying someone else’s work is not allowed.\\nAny collaboration, even at a high level, must be declared when you submit your assignment, in a note at the top of the assignment.\\nE.g., “I discussed high-level strategies for solving problem 2 and 5 with Alex.”\\nThe official references for the course are listed in the schedule below.\\nCopying from references other than these is not allowed.\\nIn particular, code and proofs should not be copied from other sources,\\nincluding Stack Overflow and other public sources.\\nStudents caught copying work are eligible for immediate failure of the course and disciplinary action by the University.\\nAll academic integrity misconduct will be treated according to UVM’s Code of Academic Integrity (url_35).\\nFinal Projects\\nThe course will include a final project, completed in groups of 1-3 students.\\nThe final project will demonstrate your mastery of the concepts covered in this course\\nby implementing a practical system to perform privacy-preserving analysis of realistic data.\\nClick here (url_36) for more complete information.\\nSchedule\\nNote that class will not be held on the following dates:\\nMonday, September 2 (Labor Day)\\nFriday, October 11 (Fall Recess)\\nNovember 25-29 (Thanksgiving)\\nImportant due dates:\\nHomework assignments are due every Monday at 11:59pm.\\nIn-class weekly exercises are due every Friday, by 11:59pm.\\nExam dates:\\nMidterm exam: Wednesday, October 9, during class (Dewey 314)\\nFinal exam: December 10, 1:30pm - 2:30pm (Dewey 314)\\nHomework dates:\\nItem\\nDue Date\\nHomework 1 (url_37)\\n9/9/24\\nHomework 2 (url_38)\\n9/16/24\\nHomework 3 (url_39)\\n9/23/24\\nHomework 4 (url_40)\\n9/30/24\\nHomework 5 (url_41)\\n10/14/24\\nHomework 6 (url_42)\\n10/21/24\\nHomework 7 (url_43)\\n10/28/24\\nHomework 8 (url_44)\\n11/4/24\\nHomework 9 (url_45)\\n11/11/24\\nHomework 10 (url_46)\\n11/18/24\\nProject proposals\\n11/22/24\\nFinal project writeup/video/implementation\\n12/9/24\\nSchedule of topics:\\nWeek of…\\nTopics\\nReference\\n8/26/24\\nIntro to data privacy; de-identification; re-identification (no exercise)\\nCh. 1\\n9/2/24\\nk-Anonymity and l-Diversity (no class Monday)\\nCh. 2\\n9/9/24\\nIntro to differential privacy; Laplace mechanism\\nCh. 3\\n9/16/24\\nSensitivity; post-processing; composition & privacy budget; unit of privacy\\nCh. 4, 5\\n9/23/24\\nClipping; approximate DP; Advanced composition; Gaussian mechanism\\nCh. 6\\n9/30/24\\nLocal sensitivity; propose-test-release, smooth sensitivity, sample-and-aggregate\\nCh. 7\\n10/7/24\\nIntermission. Review (no exercise; exam Wednesday; no class Friday)\\nNone\\n10/14/24\\nRecent variants of differential privacy\\nCh. 8\\n10/21/24\\nExponential mechanism; sparse vector technique\\nCh. 9, 10\\n10/28/24\\nPrivacy-preserving machine learning; differentially private SGD\\nCh. 12\\n11/4/24\\nLocal differential privacy\\nCh. 13\\n11/11/24\\nDifferentially private synthetic data\\nCh. 14\\n11/18/24\\nPrivacy in deep learning; Practical systems for privacy\\n\\xa0\\n11/25/24\\nNo class (Thanksgiving)\\n\\xa0\\n12/2/24\\nOpen challenges; review\\n\\xa0\\nAccommodations\\nIn keeping with University policy, any student with a documented\\ndisability interested in utilizing accommodations should contact SAS,\\nthe office of Disability Services on campus. SAS works with students\\nand faculty in an interactive process to explore reasonable and\\nappropriate accommodations, which are communicated to faculty in an\\naccommodation letter. All students are strongly encouraged to meet\\nwith their faculty to discuss the accommodations they plan to use in\\neach course. A student’s accommodation letter lists those\\naccommodations that will not be implemented until the student meets\\nwith their faculty to create a plan. Contact SAS: A170 Living/Learning\\nCenter; 802-656-7753; access@uvm.edu; or www.uvm.edu/access\\nReligious Holidays\\nStudents have the right to practice the religion of their choice. Each\\nsemester students should submit in writing to their instructors by the\\nend of the second full week of classes their documented religious\\nholiday schedule for the semester. An arrangement can then be made to\\nmake up the missed work.\\nStudent Athletes\\nIn order to be excused from classes, student athletes should submit\\nappropriate documentation to the Professor in advance of all\\nscheduling conflicts within the first two weeks of class. Those\\nmissing class are expected to submit make-up assignments within a\\nreasonable time period.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllabus_with_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cf1f3-fe93-4d23-84a7-f26bcb729f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Path to your WebDriver (e.g., ChromeDriver)\n",
    "webdriver_path = '/path/to/chromedriver'\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "driver = webdriver.Chrome(executable_path=webdriver_path)\n",
    "\n",
    "# URL to capture\n",
    "url = 'https://www.example.com'\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Get the dimensions of the webpage (full height)\n",
    "page_width = driver.execute_script('return document.body.scrollWidth')\n",
    "page_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "# Set the window size to match the full page dimensions\n",
    "driver.set_window_size(page_width, page_height)\n",
    "\n",
    "# Save the full-page screenshot\n",
    "screenshot_path = 'full_screenshot.png'\n",
    "driver.save_screenshot(screenshot_path)\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Full-page screenshot saved to {screenshot_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
